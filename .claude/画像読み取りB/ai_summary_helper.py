# -*- coding: utf-8 -*-
import sys
import re
import json
from datetime import datetime
import collections

class AISummaryHelper:
    """
    é«˜åº¦ãªAIè¦ç´„ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
    ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§å‹•ä½œã™ã‚‹é«˜åº¦ãªæ–‡ç« è¦ç´„æ©Ÿèƒ½
    """
    
    def __init__(self):
        # æ–‡ç« æ§‹é€ ã®é‡è¦åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.structure_weights = {
            'title': 5.0,      # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»è¦‹å‡ºã—
            'first_sentence': 3.0,  # æœ€åˆã®æ–‡
            'last_sentence': 2.5,   # æœ€å¾Œã®æ–‡
            'question': 3.0,    # ç–‘å•æ–‡
            'conclusion': 4.0,  # çµè«–ã‚’ç¤ºã™è¡¨ç¾
            'emphasis': 2.5,    # å¼·èª¿è¡¨ç¾
            'data': 3.5,        # æ•°å€¤ãƒ»ãƒ‡ãƒ¼ã‚¿
            'action': 3.0,      # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å‹•è©
        }
        
        # é«˜åº¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†é¡
        self.keyword_categories = {
            'business': ['å£²ä¸Š', 'åˆ©ç›Š', 'æå¤±', 'å¥‘ç´„', 'å–å¼•', 'ä¼šç¤¾', 'ä¼æ¥­', 'äº‹æ¥­', 'å–¶æ¥­', 'è²©å£²', 'é¡§å®¢', 'å¸‚å ´', 'ç«¶åˆ', 'æˆ¦ç•¥', 'è¨ˆç”»', 'äºˆç®—', 'æŠ•è³‡', 'è³‡é‡‘'],
            'time': ['ä»Šæ—¥', 'æ˜æ—¥', 'æ˜¨æ—¥', 'ä»Šé€±', 'æ¥é€±', 'ä»Šæœˆ', 'æ¥å¹´', 'ç· åˆ‡', 'æœŸé™', 'äºˆå®š', 'é–‹å§‹', 'çµ‚äº†', 'å®Œäº†', 'é…å»¶', 'æ€¥ã', 'è‡³æ€¥'],
            'people': ['ç¤¾é•·', 'éƒ¨é•·', 'èª²é•·', 'ä¸»ä»»', 'æ‹…å½“', 'è²¬ä»»è€…', 'ç®¡ç†è€…', 'ãƒªãƒ¼ãƒ€ãƒ¼', 'ãƒãƒ¼ãƒ ', 'ã‚¹ã‚¿ãƒƒãƒ•', 'å¾“æ¥­å“¡', 'è·å“¡', 'å°‚é–€å®¶', 'é¡§å®¢', 'å–å¼•å…ˆ'],
            'action': ['å®Ÿæ–½', 'å®Ÿè¡Œ', 'å®Ÿç¾', 'é”æˆ', 'å®Œæˆ', 'ä½œæˆ', 'åˆ¶ä½œ', 'é–‹ç™º', 'æ”¹å–„', 'æ”¹è‰¯', 'ä¿®æ­£', 'å¤‰æ›´', 'è¿½åŠ ', 'å‰Šé™¤', 'æ›´æ–°', 'ç¢ºèª', 'æ¤œè¨', 'æ¤œè¨¼', 'åˆ†æ'],
            'status': ['æˆåŠŸ', 'å¤±æ•—', 'å®Œäº†', 'æœªå®Œäº†', 'é€²è¡Œä¸­', 'å»¶æœŸ', 'ä¸­æ­¢', 'æ‰¿èª', 'å¦æ±º', 'ä¿ç•™', 'æ¤œè¨ä¸­', 'ç¢ºå®š', 'æ±ºå®š', 'å¤‰æ›´'],
            'importance': ['é‡è¦', 'å¿…é ˆ', 'å¿…è¦', 'ä¸è¦', 'ç·Šæ€¥', 'å„ªå…ˆ', 'é‡å¤§', 'æ·±åˆ»', 'critical', 'important', 'urgent', 'æ³¨æ„', 'è­¦å‘Š'],
            'emotion': ['æº€è¶³', 'ä¸æº€', 'å®‰å¿ƒ', 'å¿ƒé…', 'æœŸå¾…', 'ä¸å®‰', 'å–œã³', 'æ‚²ã—ã¿', 'é©šã', 'å›°æƒ‘', 'èˆˆå‘³', 'é–¢å¿ƒ']
        }
        
        # æ–‡ç« ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        self.sentence_patterns = {
            'conclusion': [r'ã—ãŸãŒã£ã¦', r'ãã®ãŸã‚', r'çµè«–ã¨ã—ã¦', r'ã¾ã¨ã‚ã‚‹ã¨', r'ã¤ã¾ã‚Š', r'è¦ã™ã‚‹ã«', r'ã“ã®ã‚ˆã†ã«'],
            'cause_effect': [r'ãªãœãªã‚‰', r'ç†ç”±ã¯', r'åŸå› ã¯', r'çµæœã¨ã—ã¦', r'ãã®çµæœ', r'å½±éŸ¿ã§'],
            'contrast': [r'ã—ã‹ã—', r'ã¨ã“ã‚ãŒ', r'ä¸€æ–¹', r'åå¯¾ã«', r'ãã‚Œã«å¯¾ã—ã¦', r'é€†ã«'],
            'addition': [r'ã¾ãŸ', r'ã•ã‚‰ã«', r'åŠ ãˆã¦', r'ãã®ä¸Š', r'åŒæ™‚ã«'],
            'example': [r'ä¾‹ãˆã°', r'å…·ä½“çš„ã«ã¯', r'ãŸã¨ãˆã°', r'å®Ÿéš›ã«'],
            'emphasis': [r'ç‰¹ã«', r'ã¨ã‚Šã‚ã‘', r'ä½•ã‚ˆã‚Š', r'æœ€ã‚‚', r'éå¸¸ã«', r'æ¥µã‚ã¦']
        }
    
    def analyze_text_structure(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã®æ§‹é€ ã‚’åˆ†æ"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text.strip())
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]
        
        if not sentences:
            return {}
        
        structure_info = {
            'total_sentences': len(sentences),
            'avg_sentence_length': sum(len(s) for s in sentences) / len(sentences),
            'sentences': []
        }
        
        for i, sentence in enumerate(sentences):
            sentence_info = {
                'text': sentence,
                'position': i,
                'length': len(sentence),
                'is_first': i == 0,
                'is_last': i == len(sentences) - 1,
                'has_numbers': bool(re.search(r'\d+', sentence)),
                'is_question': sentence.endswith('ï¼Ÿ') or sentence.endswith('?'),
                'keywords': [],
                'patterns': [],
                'importance_score': 0
            }
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
            for category, keywords in self.keyword_categories.items():
                found = [kw for kw in keywords if kw in sentence]
                if found:
                    sentence_info['keywords'].extend([(kw, category) for kw in found])
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            for pattern_type, patterns in self.sentence_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, sentence):
                        sentence_info['patterns'].append(pattern_type)
            
            structure_info['sentences'].append(sentence_info)
        
        return structure_info
    
    def calculate_advanced_scores(self, structure_info):
        """é«˜åº¦ãªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        for sentence_info in structure_info['sentences']:
            score = 0
            
            # ä½ç½®ã«ã‚ˆã‚‹é‡ã¿
            if sentence_info['is_first']:
                score += self.structure_weights['first_sentence']
            if sentence_info['is_last']:
                score += self.structure_weights['last_sentence']
            
            # ç–‘å•æ–‡ã®é‡ã¿
            if sentence_info['is_question']:
                score += self.structure_weights['question']
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹é‡ã¿
            keyword_score = 0
            for keyword, category in sentence_info['keywords']:
                if category == 'importance':
                    keyword_score += 3.0
                elif category == 'business':
                    keyword_score += 2.5
                elif category == 'action':
                    keyword_score += 2.0
                elif category in ['time', 'people']:
                    keyword_score += 1.5
                else:
                    keyword_score += 1.0
            
            score += min(keyword_score, 10.0)  # æœ€å¤§10ç‚¹ã¾ã§
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹é‡ã¿
            for pattern in sentence_info['patterns']:
                if pattern == 'conclusion':
                    score += self.structure_weights['conclusion']
                elif pattern == 'emphasis':
                    score += self.structure_weights['emphasis']
                elif pattern in ['cause_effect', 'contrast']:
                    score += 2.0
                else:
                    score += 1.0
            
            # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿
            if sentence_info['has_numbers']:
                score += self.structure_weights['data']
            
            # æ–‡ã®é•·ã•ã«ã‚ˆã‚‹èª¿æ•´ï¼ˆé©åº¦ãªé•·ã•ãŒæœ›ã¾ã—ã„ï¼‰
            length = sentence_info['length']
            if 20 <= length <= 80:
                score += 2.0
            elif 10 <= length <= 120:
                score += 1.0
            elif length < 10:
                score -= 1.0
            
            sentence_info['importance_score'] = score
        
        return structure_info
    
    def extractive_ai_summary(self, text, max_sentences=3, diversity_factor=0.3):
        """é«˜åº¦ãªæŠ½å‡ºçš„è¦ç´„"""
        structure_info = self.analyze_text_structure(text)
        if not structure_info['sentences']:
            return "è¦ç´„ã§ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        structure_info = self.calculate_advanced_scores(structure_info)
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sentences_by_score = sorted(structure_info['sentences'], 
                                  key=lambda x: x['importance_score'], reverse=True)
        
        # å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ãŸé¸æŠ
        selected_sentences = []
        used_keywords = set()
        
        for sentence_info in sentences_by_score:
            if len(selected_sentences) >= max_sentences:
                break
            
            # å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
            sentence_keywords = set(kw for kw, cat in sentence_info['keywords'])
            
            if not selected_sentences:  # æœ€åˆã®æ–‡ã¯å¿…ãšé¸æŠ
                selected_sentences.append(sentence_info)
                used_keywords.update(sentence_keywords)
            else:
                # æ—¢å­˜ã®é¸æŠæ–‡ã¨ã®é¡ä¼¼åº¦ã‚’ãƒã‚§ãƒƒã‚¯
                overlap = len(sentence_keywords & used_keywords)
                diversity_score = 1 - (overlap / max(len(sentence_keywords), 1))
                
                if diversity_score >= diversity_factor:
                    selected_sentences.append(sentence_info)
                    used_keywords.update(sentence_keywords)
        
        # å…ƒã®é †åºã§ä¸¦ã³æ›¿ãˆ
        selected_sentences.sort(key=lambda x: x['position'])
        
        # è¦ç´„æ–‡ã‚’æ§‹ç¯‰
        summary_parts = []
        for sentence_info in selected_sentences:
            summary_parts.append(sentence_info['text'])
        
        return 'ã€‚'.join(summary_parts) + 'ã€‚'
    
    def abstractive_ai_summary(self, text):
        """æŠ½è±¡çš„è¦ç´„ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰"""
        structure_info = self.analyze_text_structure(text)
        if not structure_info['sentences']:
            return "è¦ç´„ã§ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        # è¦ç´„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ç”Ÿæˆ
        key_info = {
            'main_topics': [],
            'actions': [],
            'people': [],
            'timeline': [],
            'numbers': [],
            'conclusions': []
        }
        
        for sentence_info in structure_info['sentences']:
            # ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
            if sentence_info['importance_score'] > 5.0:
                key_info['main_topics'].append(sentence_info['text'])
            
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º
            action_keywords = [kw for kw, cat in sentence_info['keywords'] if cat == 'action']
            if action_keywords and 'action' in sentence_info['patterns']:
                key_info['actions'].append(sentence_info['text'])
            
            # äººç‰©æŠ½å‡º
            people_keywords = [kw for kw, cat in sentence_info['keywords'] if cat == 'people']
            if people_keywords:
                key_info['people'].extend(people_keywords)
            
            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³æŠ½å‡º
            time_keywords = [kw for kw, cat in sentence_info['keywords'] if cat == 'time']
            if time_keywords:
                key_info['timeline'].extend(time_keywords)
            
            # æ•°å€¤æŠ½å‡º
            if sentence_info['has_numbers']:
                numbers = re.findall(r'\d+[%å††å¹´æœˆæ—¥æ™‚åˆ†å€‹å°åæ§˜ä»¶]?', sentence_info['text'])
                key_info['numbers'].extend(numbers)
            
            # çµè«–æŠ½å‡º
            if 'conclusion' in sentence_info['patterns']:
                key_info['conclusions'].append(sentence_info['text'])
        
        # è¦ç´„æ–‡ã®ç”Ÿæˆ
        summary_parts = []
        
        if key_info['main_topics']:
            summary_parts.append("ã€ä¸»è¦å†…å®¹ã€‘" + key_info['main_topics'][0][:100] + "...")
        
        if key_info['actions']:
            summary_parts.append("ã€å®Ÿæ–½äº‹é …ã€‘" + "ã€".join(set([kw for kw, cat in 
                                [item for sublist in [[kw for kw, cat in s_info['keywords'] if cat == 'action'] 
                                 for s_info in structure_info['sentences']] for item in sublist]]))[:50])
        
        if key_info['people']:
            unique_people = list(set(key_info['people']))[:3]
            summary_parts.append("ã€é–¢ä¿‚è€…ã€‘" + "ã€".join(unique_people))
        
        if key_info['numbers']:
            unique_numbers = list(set(key_info['numbers']))[:3]
            summary_parts.append("ã€é‡è¦æ•°å€¤ã€‘" + "ã€".join(unique_numbers))
        
        if key_info['conclusions']:
            summary_parts.append("ã€çµè«–ã€‘" + key_info['conclusions'][0][:80] + "...")
        
        return '\n'.join(summary_parts) if summary_parts else "æ§‹é€ åŒ–è¦ç´„ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    def ai_smart_summary(self, text, mode='hybrid'):
        """AIé§†å‹•ã‚¹ãƒãƒ¼ãƒˆè¦ç´„"""
        if not text or len(text.strip()) < 20:
            return "ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã‚‹ãŸã‚ã€AIè¦ç´„ã¯ä¸è¦ã§ã™ã€‚"
        
        structure_info = self.analyze_text_structure(text)
        text_complexity = self.analyze_complexity(structure_info)
        
        result_parts = []
        
        if mode in ['hybrid', 'extractive']:
            # æŠ½å‡ºçš„è¦ç´„
            max_sentences = min(3, max(1, structure_info['total_sentences'] // 3))
            extractive = self.extractive_ai_summary(text, max_sentences, diversity_factor=0.4)
            result_parts.append(f"ã€AIæŠ½å‡ºè¦ç´„ã€‘\n{extractive}")
        
        if mode in ['hybrid', 'abstractive']:
            # æŠ½è±¡çš„è¦ç´„
            abstractive = self.abstractive_ai_summary(text)
            if abstractive != "æ§‹é€ åŒ–è¦ç´„ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚":
                result_parts.append(f"ã€AIæ§‹é€ åŒ–è¦ç´„ã€‘\n{abstractive}")
        
        # è¤‡é›‘åº¦åˆ†æ
        complexity_info = f"ã€æ–‡ç« åˆ†æã€‘è¤‡é›‘åº¦: {text_complexity['level']}, é‡è¦æ–‡: {text_complexity['key_sentences']}æ–‡, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦: {text_complexity['keyword_density']:.1%}"
        result_parts.append(complexity_info)
        
        return '\n\n'.join(result_parts)
    
    def analyze_complexity(self, structure_info):
        """ãƒ†ã‚­ã‚¹ãƒˆã®è¤‡é›‘åº¦åˆ†æ"""
        if not structure_info['sentences']:
            return {'level': 'ä½', 'key_sentences': 0, 'keyword_density': 0.0}
        
        total_sentences = structure_info['total_sentences']
        high_score_sentences = sum(1 for s in structure_info['sentences'] if s['importance_score'] > 5.0)
        total_keywords = sum(len(s['keywords']) for s in structure_info['sentences'])
        avg_length = structure_info['avg_sentence_length']
        
        # è¤‡é›‘åº¦ã®åˆ¤å®š
        complexity_score = 0
        
        if total_sentences > 10:
            complexity_score += 2
        elif total_sentences > 5:
            complexity_score += 1
        
        if avg_length > 50:
            complexity_score += 2
        elif avg_length > 30:
            complexity_score += 1
        
        if total_keywords > total_sentences * 2:
            complexity_score += 1
        
        if high_score_sentences > total_sentences * 0.3:
            complexity_score += 1
        
        if complexity_score >= 5:
            level = 'é«˜'
        elif complexity_score >= 3:
            level = 'ä¸­'
        else:
            level = 'ä½'
        
        keyword_density = total_keywords / max(sum(len(s['text']) for s in structure_info['sentences']), 1)
        
        return {
            'level': level,
            'key_sentences': high_score_sentences,
            'keyword_density': keyword_density
        }

def format_ai_summary_result(summary_result, original_length):
    """AIè¦ç´„çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    formatted = f"""
{'='*50}
ğŸ¤– AIæ–‡ç« è¦ç´„çµæœ - {timestamp}
{'='*50}

ğŸ“Š åˆ†ææƒ…å ±:
- å…ƒãƒ†ã‚­ã‚¹ãƒˆé•·: {original_length}æ–‡å­—
- AIè¦ç´„å¾Œ: {len(summary_result)}æ–‡å­—
- åœ§ç¸®ç‡: {100*(1-len(summary_result)/max(original_length, 1)):.1f}%
- å‡¦ç†ã‚¿ã‚¤ãƒ—: é«˜åº¦AIè¦ç´„

ğŸ§  AIè¦ç´„å†…å®¹:
{summary_result}

{'='*50}
"""
    
    return formatted

def quick_ai_summarize(text, mode='hybrid'):
    """ã‚¯ã‚¤ãƒƒã‚¯AIè¦ç´„é–¢æ•°"""
    helper = AISummaryHelper()
    return helper.ai_smart_summary(text, mode)

# ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    test_text = """
    å½“ç¤¾ã®ç¬¬3å››åŠæœŸã®æ¥­ç¸¾ã«ã¤ã„ã¦å ±å‘Šã„ãŸã—ã¾ã™ã€‚
    å£²ä¸Šé«˜ã¯å‰å¹´åŒæœŸæ¯”ã§18%å¢—åŠ ã—ã€éå»æœ€é«˜ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚
    ç‰¹ã«æ–°å•†å“Aã®å£²ä¸ŠãŒå¥½èª¿ã§ã€ç›®æ¨™ã‚’å¤§ããä¸Šå›ã£ã¦ã„ã¾ã™ã€‚
    ä¸€æ–¹ã§ã€åŸææ–™è²»ã®é«˜é¨°ã«ã‚ˆã‚Šåˆ©ç›Šç‡ã¯è‹¥å¹²ä½ä¸‹ã—ã¾ã—ãŸã€‚
    ä»Šå¾Œã®æˆ¦ç•¥ã¨ã—ã¦ã€ã‚³ã‚¹ãƒˆå‰Šæ¸›ã¨æ–°å¸‚å ´é–‹æ‹“ã«æ³¨åŠ›ã—ã¾ã™ã€‚
    ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ¼ãƒ€ãƒ¼ã¯ç”°ä¸­éƒ¨é•·ãŒæ‹…å½“ã—ã€æ¥æœˆã‹ã‚‰é–‹å§‹äºˆå®šã§ã™ã€‚
    è©³ç´°ã¯æ¥é€±ã®ä¼šè­°ã§è­°è«–ã™ã‚‹ã“ã¨ãŒæ±ºå®šã•ã‚Œã¾ã—ãŸã€‚
    """
    
    helper = AISummaryHelper()
    
    print("=== AIè¦ç´„ãƒ†ã‚¹ãƒˆ ===")
    print("å…ƒãƒ†ã‚­ã‚¹ãƒˆ:")
    print(test_text)
    print("\nAIè¦ç´„çµæœ:")
    result = helper.ai_smart_summary(test_text)
    print(result)